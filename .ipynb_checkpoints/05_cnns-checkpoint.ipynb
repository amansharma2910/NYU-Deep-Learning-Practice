{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks\n",
    "---\n",
    "The reason why CNNs work so well is because of 2 properties observed in data:\n",
    "* __Stationarity of Statistics:__ This means given small local patches, there is a high probability of finding recurrent patterns in the locality. A simpler definition for this is that some motifs tend to reoccur within the (image) data. This allows for parameter sharing in case of CNNs, which inversely affects the number of parameters in the model, and hence allows relatively lesser training times as compared to, say, fully connected layers.\n",
    "\n",
    "* __Locality of Pixel Dependencies:__ This principal states that pixels that are close to each other tend to be more correlated and dependent on each other as compared to those far away. In simpler words, pixels that are closer to each other tend to be of similar color. This also means that related data tends to be concentrated into small patches. Locality affects that sparsity of the connections.\n",
    "\n",
    "* __Compositionality:__ Talking about in terms of image data, images are composed of smaller, simpler patterns. In fact, all data is composed of simpler data. Thus, instead of looking for a certain object within the image, the network can focus on discovering these patterns within the image. \n",
    "\n",
    "In this notebook, we will see how CNNs perform as compared to FC nerworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from res.plot_lib import *\n",
    "set_default() # setting the default plot style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing dependencies\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import dataset, DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate the number of parameters in the model\n",
    "def count_parameters(model):\n",
    "    params = 0\n",
    "    for p in list(model.parameters()):\n",
    "        params += p.nelement()\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# selecting the default device\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to E-Learning/NYU-DL/data/MNIST\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.1%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting E-Learning/NYU-DL/data/MNIST\\MNIST\\raw\\train-images-idx3-ubyte.gz to E-Learning/NYU-DL/data/MNIST\\MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to E-Learning/NYU-DL/data/MNIST\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "113.5%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting E-Learning/NYU-DL/data/MNIST\\MNIST\\raw\\train-labels-idx1-ubyte.gz to E-Learning/NYU-DL/data/MNIST\\MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to E-Learning/NYU-DL/data/MNIST\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.4%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting E-Learning/NYU-DL/data/MNIST\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to E-Learning/NYU-DL/data/MNIST\\MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to E-Learning/NYU-DL/data/MNIST\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "180.4%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting E-Learning/NYU-DL/data/MNIST\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to E-Learning/NYU-DL/data/MNIST\\MNIST\\raw\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# creating the dataloaders to feed data to the models\n",
    "train_loader = DataLoader(\n",
    "    datasets.MNIST(\"E-Learning/NYU-DL/data/MNIST\", train=True, download=True, # download = True for first time running it\n",
    "                        transform= transforms.Compose([\n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])),\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "validation_loader = DataLoader(\n",
    "    datasets.MNIST(\"E-Learning/NYU-DL/data/MNIST\", train=False, download=True, \n",
    "                        transform= transforms.Compose([\n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])),\n",
    "    batch_size=256,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeiling\n",
    "---\n",
    "### FC Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCModel(nn.Module):\n",
    "    def __init__(self, input_size, n_hidden, output_size):\n",
    "        super(self, FCModel).__init__()\n",
    "        self.input_dims = input_size\n",
    "        self.n_hidden = n_hidden\n",
    "        self.output_size = output_size \n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(self.input_size, self.n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.n_hidden, self.n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.n_hidden, self.output_size),\n",
    "            nn.LogSoftmax(dim=1),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.input_size)\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModel:\n",
    "    def __init__(self, input_size, n_channels, output_size):\n",
    "        super(self, CNNModel).__init()\n",
    "        self.input_size = input_size\n",
    "        self.n_channels = n_channels\n",
    "        self.output_size = output_size\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=n_channels//2, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(in_channels=n_channels//2, out_channels=n_channels, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(n_channels*4*4, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # input shape = m x 28 x 28 x 1\n",
    "        x = self.conv1(x) # m x 24 x 24 x n_channels//2\n",
    "        x = F.relu(x)\n",
    "        x = nn.MaxPool2d(x, kernel_size=2) # m x 12 x 12 x n_channels//2\n",
    "        x = self.conv2(x) # m x 8 x 8 x n_channels\n",
    "        x = F.relu(x)\n",
    "        x = nn.MaxPool2d(x, kernel_size=2) # 4 x 4 x n_channels\n",
    "        x = nn.Flatten(x) # m x 4 * 4 * n_channels\n",
    "        x = nn.fc1(x) # m x 50\n",
    "        x = F.relu(x)\n",
    "        x = nn.fc2(x) # m x 10\n",
    "        x = F.log_softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have defined the models, let us define the training and validation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
