{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Algebra with PyTorch\n",
    "---\n",
    "\n",
    "Let us begin by importing the necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determinant of a matrix: \n",
    "The determinant of matrix is calculated using the __torch.linalg.det()__ function. One thing to be noted is that the function accepts only a float matrix. Also, you can calculate the determinant of a square matrix only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 7.,  7., 10., 12.],\n",
      "        [10.,  2., 11.,  5.],\n",
      "        [14.,  2.,  9., 10.],\n",
      "        [12., 11.,  4., 11.]], dtype=torch.float64)\n",
      "tensor(-5743., dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# determinant of a matrix\n",
    "a = torch.randint(20, (4, 4)).double()\n",
    "print(a)\n",
    "print(torch.linalg.det(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inverse of a matrix:\n",
    "\n",
    "The torch.inverse() function is used to calculate the inverse of a matrix. One thing to be noted is that not every matrix is invertible. In case you try to invert a singular matrix (determinant = 0), you will end up with a run time error. \n",
    "\n",
    "Hence, checking if a matrix is singular is a good exception handling method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0323,  0.0832, -0.0156,  0.0447],\n",
      "        [ 0.1613, -0.0614, -0.0510, -0.3205],\n",
      "        [-0.4194,  0.0499,  0.4906,  0.7268],\n",
      "        [ 0.0968, -0.0562, -0.1145, -0.0052]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "a = torch.randint(20, (4, 4)).double()\n",
    "if torch.linalg.det(a):\n",
    "    print(torch.inverse(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "inverse_cpu: U(3,3) is zero, singular U.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-7a4af144619f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m                   \u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                   [0,0,0]])\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minverse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: inverse_cpu: U(3,3) is zero, singular U."
     ]
    }
   ],
   "source": [
    "# will result in an error as matrix 'b' is singular\n",
    "\n",
    "b = torch.Tensor([[1,2,3],\n",
    "                  [2,3,4],\n",
    "                  [0,0,0]])\n",
    "torch.inverse(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Norms:\n",
    "\n",
    "The torch.linalg.norm() method is used to calculate the various types of norms of matrices and vectors.\n",
    "\n",
    "Here are some of the most used norms within linear algebra.\n",
    "\n",
    "* __L<sup>2</sup> norm__: Used to calculate the distance of a vector from the origin.\n",
    "\n",
    "* __L<sup>1</sup> norm__: The L1 norm is used is used when the difference between zero and non-zero elements is very important. The value of the L1 norm increases *e* when the vector moves away from the origin by a value *e*.\n",
    "\n",
    "* __L<sup>0</sup> norm__: The L0 norm is used to calculate the number of non-zero elements within a vector.\n",
    "\n",
    "* __Infinity norm__: The infinity norm is used to calculate the maximum element within the vector. Similarly, the -infinity norm is used to calculate the value of the minimum element within the vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 norm = tensor(13.4536)\n",
      "L1 norm = tensor(33.)\n",
      "L0 norm = tensor(8.)\n",
      "Inf norm = tensor(9.)\n",
      "-Inf norm = tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "v = torch.Tensor([3, 4, 1, 2, 0, 6, 3, 0, 5, 9, 0])\n",
    "print('L2 norm =', torch.linalg.norm(v, ord = 2))\n",
    "print('L1 norm =', torch.linalg.norm(v, ord = 1))\n",
    "print('L0 norm =', torch.linalg.norm(v, ord = 0))\n",
    "print('Inf norm =', torch.linalg.norm(v, ord = float('inf')))\n",
    "print('-Inf norm =', torch.linalg.norm(v, ord = -float('inf')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eignedecomposition:\n",
    "\n",
    "### → A v = λ v  \n",
    "\n",
    "Eigendecomposition can be used to decompose a given square matrix into eigenvalues and eigenvectors. Doing so allows us to analyze certain properties of the matrix that otherwise might not be visible to the practitioner at the first glance of the data matrix.  \n",
    "\n",
    "Here are some of the properties of eigendecomposition:\n",
    "\n",
    "* Any real symmetric matrix is guaranteed to have an eigendecomposition. However, the eigendecomposition might not be unique. The eigendecomposition is said to be unique only and only if all the eigenvalues are unique.\n",
    "\n",
    "* If any of the eigenavlues are 0, then the matrix is a singular matrix.\n",
    "\n",
    "* In case of positive semidefinite matrices (all eigenvalues either positive or 0), __x<sup>T</sup> A x >= 0__.\n",
    "\n",
    "* In case of positive definite matrices, __x<sup>T</sup> A x = 0__.\n",
    "\n",
    "Now, let us have a look at how to perform eigendecomposition using PyTorch. \n",
    "\n",
    "NOTE: \n",
    "> * Eigendecomposition is possible only for square matrices.\n",
    "> * Regular eigendecomposition (torch.eig()) can return complex eigenvectors and eigenvalues. Hence, backpropagation is not possible in case of torch.eig. Use __torch.symeig()__ to be able to perform backpropagation on eigenvectors. Symmetric eigendecomposition is carried out for real, symmetric matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# square asymmetric matrix\n",
    "a = torch.Tensor([[1, 6, 5, 4],\n",
    "                  [7, 3, 2, 1],\n",
    "                  [3, 8, 9, 6],\n",
    "                  [4, 5, 6, 7]])\n",
    "\n",
    "# square symmetric matrix\n",
    "b = torch.Tensor([[1, 2, 3, 4, 5],\n",
    "                  [2, 3, 4, 5, 6],\n",
    "                  [3, 4, 5, 6, 7],\n",
    "                  [4, 5, 6, 7, 8],\n",
    "                  [5, 6, 7, 8, 9]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigenvalues = tensor([[19.3617,  0.0000],\n",
      "        [-3.7889,  0.0000],\n",
      "        [ 1.5183,  0.0000],\n",
      "        [ 2.9089,  0.0000]])\n",
      "Eigenvectors = tensor([[ 0.3973,  0.6052, -0.0843, -0.1020],\n",
      "        [ 0.2857, -0.7128, -0.3329, -0.5572],\n",
      "        [ 0.6634,  0.3440,  0.7931, -0.0292],\n",
      "        [ 0.5661, -0.0853, -0.5030,  0.8236]])\n"
     ]
    }
   ],
   "source": [
    "# regular eigendecomposition\n",
    "eig_val, eig_vect = torch.eig(a, eigenvectors=True)\n",
    "print('Eigenvalues =', eig_val)\n",
    "print('Eigenvectors =', eig_vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing to be noted is that in case of regular eigendecomposition, the eigenvalue is a __n x 2__ matrix. The first column of the matrices represents the real part of each eigenvalue, and the second column represents the imaginary part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigenvalues = tensor([[19.3617,  0.0000],\n",
      "        [-3.7889,  0.0000],\n",
      "        [ 1.5183,  0.0000],\n",
      "        [ 2.9089,  0.0000]])\n",
      "Eigenvectors = tensor([[ 0.3973,  0.6052, -0.0843, -0.1020],\n",
      "        [ 0.2857, -0.7128, -0.3329, -0.5572],\n",
      "        [ 0.6634,  0.3440,  0.7931, -0.0292],\n",
      "        [ 0.5661, -0.0853, -0.5030,  0.8236]])\n"
     ]
    }
   ],
   "source": [
    "# symmetric eigendecomposition\n",
    "symeig_val, symeig_vect = torch.symeig(b, eigenvectors=True)\n",
    "print('Eigenvalues =', eig_val)\n",
    "print('Eigenvectors =', eig_vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Singular Value Decomposition:\n",
    "\n",
    "### → A<sub>m x n</sub> = U<sub>m x m</sub> D<sub>m x n</sub> V<sub>n x n</sub>\n",
    "\n",
    "As Prof. Gilbert Strang said, SVD is one of the most important equations in linear algebra. Built on the same idea as eigendecomposition, SVD addresses one major issue with eigendecomposition. \n",
    "\n",
    "> Eigendecomposition is only applicable to square matrices. Also, you get real eigenvalues only for non-singular matrices. \n",
    "\n",
    "> SVD on the other hand allows decomposition of rectangular matrices as well. And unlike eigendecomposition, SVD is possible for every matrix. \n",
    "\n",
    "Let us have a look at how to perform SVD using PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[11.,  3.,  9., 10.,  7.,  0.],\n",
      "        [19.,  7., 18.,  8.,  7., 13.],\n",
      "        [ 1., 18., 11.,  7., 16.,  0.],\n",
      "        [ 5.,  8.,  7.,  8.,  8.,  3.]], dtype=torch.float64)\n",
      "\n",
      "Left singular vector =\n",
      " tensor([[-0.3929,  0.1463, -0.8671, -0.2691],\n",
      "        [-0.6667,  0.5986,  0.4332, -0.0971],\n",
      "        [-0.5184, -0.7722,  0.2002, -0.3081],\n",
      "        [-0.3639, -0.1547, -0.1428,  0.9073]], dtype=torch.float64)\n",
      "\n",
      "Singular values =\n",
      " tensor([44.2519, 19.2735,  8.0998,  2.5879], dtype=torch.float64)\n",
      "\n",
      "Right singular vector =\n",
      " tensor([[-0.4368, -0.4087, -0.5375, -0.3571, -0.4208, -0.2205],\n",
      "        [ 0.5934, -0.5452,  0.1305, -0.0203, -0.4347,  0.3797],\n",
      "        [-0.2248,  0.3570,  0.1477, -0.6107, -0.1206,  0.6424],\n",
      "        [-0.2225,  0.0876, -0.4662,  0.6317, -0.0903,  0.5642]],\n",
      "       dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# 4 x 6 matrix of floats\n",
    "A = torch.randint(20, (4, 6)).double()\n",
    "\n",
    "U, D, V = torch.svd(A)\n",
    "\n",
    "print(A)\n",
    "print('\\nLeft singular vector =\\n', U)\n",
    "print('\\nSingular values =\\n', D)\n",
    "print('\\nRight singular vector =\\n', V.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.1000e+01,  3.0000e+00,  9.0000e+00,  1.0000e+01,  7.0000e+00,\n",
       "         -4.4504e-16],\n",
       "        [ 1.9000e+01,  7.0000e+00,  1.8000e+01,  8.0000e+00,  7.0000e+00,\n",
       "          1.3000e+01],\n",
       "        [ 1.0000e+00,  1.8000e+01,  1.1000e+01,  7.0000e+00,  1.6000e+01,\n",
       "         -1.1131e-14],\n",
       "        [ 5.0000e+00,  8.0000e+00,  7.0000e+00,  8.0000e+00,  8.0000e+00,\n",
       "          3.0000e+00]], dtype=torch.float64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_regenerated = torch.mm(torch.mm(U, torch.diag(D)), V.t())\n",
    "A_regenerated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudoinverse: \n",
    "\n",
    "### → A<sup>+</sup> = V D<sup>+</sup> U<sup>T</sup>\n",
    "### → x = A<sup>+</sup> y\n",
    "(Denoted by A<sup>+</sup> for a rectangular matrix A)\n",
    "\n",
    "While inverses can only be calculated for square, non-singular matrices, in many real-world cases, the matrices that we will be working with are generally rectangular in nature. \n",
    "\n",
    "One of the main advantages of having the inverse of a matrix A is that it allows us to solve the eqation __A x = y__ quite easily as __x = A<sup>-</sup> y__.\n",
    "\n",
    "Thus, the concept of pseudoinverse in an inmportant one while working with matrices. \n",
    "\n",
    "While pseudoinverses will not give us the exact results, however, it ensures that the L<sub>2</sub> norm of the vector x (||x||<sub>2</sub>) will be minimum.\n",
    "\n",
    "Here is to calculate pseudoinverses using PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.0000e+00, -3.4694e-17, -2.0817e-17,  1.6653e-16,  7.3726e-18],\n",
      "        [-2.4286e-16,  1.0000e+00, -4.6491e-16, -2.4980e-16,  4.0289e-16],\n",
      "        [ 4.1633e-17, -9.7145e-17,  1.0000e+00, -3.7470e-16,  1.8995e-16],\n",
      "        [-1.1796e-16,  6.9389e-18,  1.0408e-16,  1.0000e+00,  5.4600e-16],\n",
      "        [-1.3878e-16,  8.3267e-17, -8.3267e-17, -6.9389e-17,  1.0000e+00]],\n",
      "       dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "A = torch.randint(30, (5,6)).double()\n",
    "\n",
    "A_plus = torch.pinverse(A)\n",
    "\n",
    "# non-diagonal elements approach 0\n",
    "print(A @ A_plus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis\n",
    "\n",
    "One of the few uses of PCA is lossy compression of a matrix of data. What happens in the process of 'lossy compression, is that a matrix __A__ <sub>R<sup> M</sup></sub> is encoded into a matrix __C__ <sub>R<sup> L</sup></sub> such that L < M. This reduces the dimensionality of the matrix A, thus taking up less space within the memory.\n",
    "\n",
    "However, one drawback is that upon decompression, some of the information might be lost, hence the term 'lossy'.\n",
    "\n",
    "The following is the process of compression and decompression:\n",
    "\n",
    "#### → C = f(A)\n",
    "#### → A ≈ g(f(A))\n",
    "\n",
    "Here, __f__ denotes the encoding function, and __g__ denotes the decoding function. \n",
    "\n",
    "The main idea behind PCA is to map a n-dimensional data to k-dimensions, such that __k < n__.\n",
    "\n",
    "The newly obtained k-dimensional matrix is an orthogonal matrix.\n",
    "\n",
    "The following method is used to implement SVD in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[12.,  3., 47., 38., 48.],\n",
       "        [ 0., 42., 29.,  1.,  8.],\n",
       "        [48., 21., 19., 43.,  3.],\n",
       "        [ 9., 48.,  8.,  5., 13.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randint(50, (4,5)).double()\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-6.7462e-01, -5.2593e-01,  1.3526e-01,  5.0000e-01],\n",
      "        [ 4.5018e-01, -2.7839e-01, -6.8545e-01,  5.0000e-01],\n",
      "        [-2.8593e-01,  8.0368e-01, -1.4947e-01,  5.0000e-01],\n",
      "        [ 5.1036e-01,  6.3415e-04,  6.9966e-01,  5.0000e-01]],\n",
      "       dtype=torch.float64)\n",
      "tensor([6.0807e+01, 4.6835e+01, 1.4705e+01, 5.6597e-28], dtype=torch.float64)\n",
      "tensor([[-0.2833,  0.6890,  0.0507, -0.6269],\n",
      "        [ 0.5818,  0.0777,  0.1402,  0.1135],\n",
      "        [-0.3289, -0.3740, -0.7320, -0.1680],\n",
      "        [-0.5744,  0.3053,  0.1037,  0.7007],\n",
      "        [-0.3783, -0.5349,  0.6567, -0.2738]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "u, s, v = torch.pca_lowrank(x)\n",
    "print(u)\n",
    "print(s)\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, projecting the matrix __x__ to first __k__ principal components can be done by: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-57.1003, -23.1516,   2.0869],\n",
      "        [ 11.2952, -11.5581,  -9.9812],\n",
      "        [-33.4655,  39.1209,  -2.1000],\n",
      "        [ 14.9547,   1.5100,  10.3862]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# let k = 3\n",
    "x_k = torch.matmul(x, v[:, :3])\n",
    "print(x_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
