{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks\n",
    "---\n",
    "The reason why CNNs work so well is because of 2 properties observed in data:\n",
    "* __Stationarity of Statistics:__ This means given small local patches, there is a high probability of finding recurrent patterns in the locality. A simpler definition for this is that some motifs tend to reoccur within the (image) data. This allows for parameter sharing in case of CNNs, which inversely affects the number of parameters in the model, and hence allows relatively lesser training times as compared to, say, fully connected layers.\n",
    "\n",
    "* __Locality of Pixel Dependencies:__ This principal states that pixels that are close to each other tend to be more correlated and dependent on each other as compared to those far away. In simpler words, pixels that are closer to each other tend to be of similar color. This also means that related data tends to be concentrated into small patches. Locality affects that sparsity of the connections.\n",
    "\n",
    "* __Compositionality:__ Talking about in terms of image data, images are composed of smaller, simpler patterns. In fact, all data is composed of simpler data. Thus, instead of looking for a certain object within the image, the network can focus on discovering these patterns within the image. \n",
    "\n",
    "In this notebook, we will see how CNNs perform as compared to FC nerworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from res.plot_lib import *\n",
    "set_default() # setting the default plot style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing dependencies\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import dataset, DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate the number of parameters in the model\n",
    "def count_parameters(model):\n",
    "    params = 0\n",
    "    for p in list(model.parameters()):\n",
    "        params += p.nelement()\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# selecting the default device\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the dataloaders to feed data to the models\n",
    "train_loader = DataLoader(\n",
    "    datasets.MNIST(\"E-Learning/NYU-DL/data/MNIST\", train=True, download=True, \n",
    "                        transform= transforms.Compose([\n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])),\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "validation_loader = DataLoader(\n",
    "    datasets.MNIST(\"E-Learning/NYU-DL/data/MNIST\", train=False, download=True, \n",
    "                        transform= transforms.Compose([\n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])),\n",
    "    batch_size=256,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeiling\n",
    "---\n",
    "### FC Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size  = 28*28  \n",
    "output_size = 10  \n",
    "\n",
    "class FCModel(nn.Module):\n",
    "    def __init__(self, input_size, n_hidden, output_size):\n",
    "        super(FCModel, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.n_hidden = n_hidden\n",
    "        self.output_size = output_size \n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(self.input_size, self.n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.n_hidden, self.n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.n_hidden, self.output_size),\n",
    "            nn.LogSoftmax(dim=1),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.input_size)\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, n_channels, output_size):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.output_size = output_size\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=n_channels//2, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(in_channels=n_channels//2, out_channels=n_channels, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(n_channels*4*4, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # input shape = m x 28 x 28 x 1\n",
    "        x = self.conv1(x) # m x 24 x 24 x n_channels//2\n",
    "        x = F.relu(x)\n",
    "        x = nn.MaxPool2d(kernel_size=2)(x) # m x 12 x 12 x n_channels//2\n",
    "        x = self.conv2(x) # m x 8 x 8 x n_channels\n",
    "        x = F.relu(x)\n",
    "        x = nn.MaxPool2d(kernel_size=2)(x) # 4 x 4 x n_channels\n",
    "        x = nn.Flatten()(x) # m x 4 * 4 * n_channels\n",
    "        x = self.fc1(x) # m x 50\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x) # m x 10\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have defined the models, let us define the training and validation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_list = []\n",
    "\n",
    "def train(epochs, model, perm=torch.arange(0, 784).long()):\n",
    "    model.train()\n",
    "    for batch_idx, (data, label) in enumerate(train_loader):\n",
    "        # moving the data and label to device\n",
    "        data, label = data.to(device), label.to(device)\n",
    "        # permute pixels\n",
    "        data = data.view(-1, 28*28)\n",
    "        data = data[:, perm]\n",
    "        data = data.view(-1, 1, 28, 28)\n",
    "        # resetting gradients\n",
    "        optimizer.zero_grad()\n",
    "        # training the model\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            \n",
    "\n",
    "def test(model, perm=torch.arange(0, 784).long()):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, label in validation_loader:\n",
    "        # send to device\n",
    "        data, label = data.to(device), label.to(device)\n",
    "        \n",
    "        # permute pixels\n",
    "        data = data.view(-1, 28*28)\n",
    "        data = data[:, perm]\n",
    "        data = data.view(-1, 1, 28, 28)\n",
    "        \n",
    "        output = model(data)\n",
    "        test_loss += F.nll_loss(output, label, reduction='sum').item() # sum up batch loss                                                               \n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability                                                                 \n",
    "        correct += pred.eq(label.data.view_as(pred)).cpu().sum().item()\n",
    "\n",
    "    test_loss /= len(validation_loader.dataset)\n",
    "    accuracy = 100. * correct / len(validation_loader.dataset)\n",
    "    accuracy_list.append(accuracy)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(validation_loader.dataset),\n",
    "        accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Models\n",
    "---\n",
    "\n",
    "### FC Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 6442\n",
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.311193\n",
      "Train Epoch: 0 [6400/60000 (11%)]\tLoss: 1.941585\n",
      "Train Epoch: 0 [12800/60000 (21%)]\tLoss: 1.266683\n",
      "Train Epoch: 0 [19200/60000 (32%)]\tLoss: 0.976782\n",
      "Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.615117\n",
      "Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.613135\n",
      "Train Epoch: 0 [38400/60000 (64%)]\tLoss: 0.459241\n",
      "Train Epoch: 0 [44800/60000 (75%)]\tLoss: 0.421389\n",
      "Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.625438\n",
      "Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.425138\n",
      "\n",
      "Test set: Average loss: 0.4281, Accuracy: 8719/10000 (87%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n_hidden = 8 # number of hidden units\n",
    "\n",
    "fc_model = FCModel(input_size, n_hidden, output_size)\n",
    "fc_model.to(device)\n",
    "optimizer = optim.SGD(fc_model.parameters(), lr=0.01, momentum=0.5)\n",
    "print('Number of parameters: {}'.format(count_parameters(fc_model)))\n",
    "\n",
    "for epoch in range(0, 1):\n",
    "    train(epoch, fc_model)\n",
    "    test(fc_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ConvNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 5894\n",
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.317034\n",
      "Train Epoch: 0 [6400/60000 (11%)]\tLoss: 1.833360\n",
      "Train Epoch: 0 [12800/60000 (21%)]\tLoss: 0.761588\n",
      "Train Epoch: 0 [19200/60000 (32%)]\tLoss: 0.465877\n",
      "Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.475990\n",
      "Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.238010\n",
      "Train Epoch: 0 [38400/60000 (64%)]\tLoss: 0.347201\n",
      "Train Epoch: 0 [44800/60000 (75%)]\tLoss: 0.240683\n",
      "Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.325439\n",
      "Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.190021\n",
      "\n",
      "Test set: Average loss: 0.2056, Accuracy: 9361/10000 (94%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training settings \n",
    "n_channels = 6 # number of feature maps\n",
    "\n",
    "cnn_model = CNNModel(n_channels, output_size)\n",
    "cnn_model.to(device)\n",
    "optimizer = optim.SGD(cnn_model.parameters(), lr=0.01, momentum=0.5)\n",
    "print('Number of parameters: {}'.format(count_parameters(cnn_model)))\n",
    "\n",
    "for epoch in range(0, 1):\n",
    "    train(epoch, cnn_model)\n",
    "    test(cnn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
